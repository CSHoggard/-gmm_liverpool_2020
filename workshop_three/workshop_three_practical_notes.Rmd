---
title: "Geometric Morphometrics and Archaeological Science"
subtitle: "Workshop Three (July 2020)"
author: "Dr. Christian Steven Hoggard (University of Southampton, United Kingdom)"
output:
  html_document: default
  pdf_document:
       latex_engine: xelatex 
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

## **Introductory remarks**

This guide provides a "hands-on" step-by-step introduction into the application of geometric morphometric (GMM) methodologies in archaeological science (as conducted through the R Environment). This guide will **provide an overview into analysing outline (open and closed) data through two case studies**.

Like the previous workshop, we will spend roughly 3-5 minutes per 'chunk'. Chunks can be used as a means of rendering R output into documents, or to simply display code for illustration. The chunks presented here will minimise error resulting from manual input and ensure all participants are at the same stage. To run a 'chunk' (displayed as a shaded area and representing a function or suite of actions), we can press the "Run Selected Chunk" button, represented by a play button, or alternatively use the shortcut `ctrl + enter` on the highlighted code.  

For participants: **When you complete a function please use a "thumbs up" emoji on Slack. If there is an issue please raise your query in the Zoom Chat**. We are allowing time between functions to ensure that all participants (of varying R knowledge) can keep up; if you finish a particular process early please explore the functions in the packages used throughout this workshop, or individual functions through the 'Help' tab in the 'Packages' window.

This practical constitutes the third and final workshop on GMM and Archaeological Science, organised by Lucy Timbrell and Christopher Scott and led by Dr. Christian Steven Hoggard. This markdown, along with all resources (and recordings of each workshop) can be found on the workshop GitHub repository: https://github.com/CSHoggard/-gmm_liverpool_2020.

We thank the Arts and Humanities Research Council North West Consortium Doctoral Training Partnership (AHRC NWCDTP) for supporting this workshop.


## **About the Code, Packages and Files**

We will examine one case-study throughout this practical:

Inovaite et al. (2020). All these Fantastic Cultures? Research History and Regionalization in the Late Palaeolithic Tanged Point Cultures of Eastern Europe, *European Journal of Archaeology*,  23 (2): 162-185. (https://doi.org/10.1017/eaa.2019.59).

The complete repository for this dataset can be found in a stand-alone GitHub (https://github.com/CSHoggard/-Eastern-Europe-Tanged-Points) repository, in addition to the Open Science Framework (https://osf.io/agrwb/).

For this final practical, three packages are required:  

* **Momocs** v.1.3.0 (https://cran.r-project.org/web/packages/Momocs/index.html)  

* **tidyverse** v.1.3.0 (https://cran.r-project.org/web/packages/tidyverse/index.html)  

* **rio** v.0.5.16 (https://cran.r-project.org/web/packages/rio/index.html)  

Similarly to the last practical we will be using the rio package to import our data into the environment. We therefore will not be required to set the working directory.

With R and RStudio installed, and this markdown file opened within RStudio (through `File` -> `Open file`), we need to install the aforementioned packages. For this workshop we will install these packages through the below 'chunk':

```{r chunk1, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
if(!require("Momocs")) install.packages('Momocs', repos='http://cran.us.r-project.org')  
if(!require("tidyverse")) install.packages('tidyverse', repos='http://cran.us.r-project.org')
if(!require("rio")) install.packages('rio', repos='http://cran.us.r-project.org')
```

As the tidyverse and Momocs packages may take time to install given the size of the files *please ensure that these are downloaded prior the workshop*. Once installed we can now activate and use these packages through the `library()` function. 

```{r chunk2, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(Momocs)
library(rio)
library(tidyverse)
```


## **About the Data**  

This data was composed to assess the robustness of cultural taxonomies in the Final Palaeolithic period of Eastern Europe, as portrayed through tanged point variants. 250 illustrations of tanged points were synthesised into one .tps file (using tpsUtil) before being digitised in tpsDig2 (https://life.bio.sunysb.edu/morph/soft-dataacq.html). Through the **outline object** function all tanged points were converted into outlines (a series of semi-landmarks, equidistant and placed using an algorithm, and of varying size). A database of all examples and their respective cultural assignment is also provided.  


## **Creating and Importing GMM Data: Alternative Approaches**

There are a number of ways with which outline data can be created. Here, all examples were digitised in tpsDig2 however functions in Momocs can also be used to extract outlines from silhouette data (black shape on white background). These are the `Momocs::import_jpg()` and `Momocs::import_jpg1()` functions, derived from the `import_conte()` function translated into R by Claude (2008). Open outlines can be digitised through an amended `import_conte()` argument.  

There are also a number of different methods with which outline data can be imported into the R Environment. Here, for ease and replicability, the outline data (in .tps format) was stored on a GitHub repository and directly fed into the R environment, utilising the `Momocs::import_tps()` function in a .rds file. Other ways to import .tps data (if saved locally) include the above function, `geomorph::readland.tps()` and rewriting tools in Momocs e.g. `Momocs::rw_rule()`. Data from stereomorph can also be imported through the `Momocs::import_StereoMorph_ldk()` and `Momocs:import_StereoMorph_curve()` functions. The package `imager` is particularly impressive in readying images for outline extraction e.g. thresholding.


## **Stage 1: Importing the GMM Data into the R Environment**

Let's import the tps outline data and the dataset from the GitHub workshop repository using the `rio::import()` functions:

```{r, chunk3, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

tpsdata <- rio::import("https://github.com/CSHoggard/-gmm_liverpool_2020/raw/master/workshop_three/tpslines.rds")

database <- rio::import("https://github.com/CSHoggard/-gmm_liverpool_2020/raw/master/workshop_three/database.rds")   

```


## **Stage 2: Data Cleaning**  

With both objects now in the R Environment we can call our tpsdata object through the `base:: View` functions (however given the size of the object it is best advised to not use this argument). The `base::View()` function will highlight the three constituent parts of the tps file: the 1) *Coo* (coordinate data), 2) *cur* (the curve data if necessary), and 3) *scale* (the scale data if present). This echoes the structure of the first case study in the second workshop.  

We can inspect our database using the `utils::head()` function, this will highlight the first six rows of the base.  

```{r chunk4, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
head(database)
```

We can observe that the data we wish to examine, *Archaeological_Unit* is `<chr>`, that is to say of type 'character' and not `<fctr>` ('factor'), the class which is required for our analysis. This can be corrected through the `base::as.factor()` function, as follows:

```{r chunk5, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
database$Archaeological_Unit <- as.factor(database$Archaeological_Unit)

is.factor(database$Archaeological_Unit)
```

Note: The factors can also be added during data importing through *readr* functions.

We can also inspect the number of different archaeological units within our dataset through the `base::summary()` function. This highlights the number of tanged points in each group. With certain taxonomic units rarely used this is reflected in the low sample sizes for certain groups e.g. Vyshegorian.  

```{r chunk6, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
summary(database$Archaeological_Unit)
```


## **Stage 3: Creation of the "Out" object**  

Central to Momocs are a specific suite of shape classes for: 1) *outlines* (OutCoo), *open outlines* (OpnCoo) and *landmarks* (LdkCoo), with often one class specific to your own dataset. Previous we wanted our objects to be of class LdkCoo, in this instance however our tps data is comprised of outlines, and so we wish for our data to be `OutCoo`, as to enable efourier (elliptic Fourier) analyses. Other analyses including rfourier (radii Fourier) or tfourier (tangent angle Fourier) analyses can be conducted through this process, however for this workshop we're only going consider elliptic Fourier analysis (EFA).

In this instance the coordinate data (coo) is transformed into outline data through the `Momocs::Out()`. When performed we can then enter the object (here titled 'shape') and examine its properties:  

```{r chunk7, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
shape <- Out(tpsdata$coo, fac = database) 
shape
```

When we call our shape we see we are investigating 250 outlines, with a mean number of 1543 landmarks, and 10 different classifiers (including the factor we want to consider).  

Note: It is important to note that the order of the specimens paralleled the database, thus rearrangement was unnecessary. In instances where rearrangement is required, the `base::list()`, `base::match()` and `dplyr::arrange()` functions are particularly helpful.   


## **Stage 4: Outline Visualisation**  

Now our data is in the R Environment and formatted appropriately for the Momocs package we can examine the outline shapes.We can first look at all outlines through the `Momocs::panel()` function. Factors can also be visualised in using the `fac` argument. For example:  

```{r chunk8, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 5, fig.height = 5}
panel(shape, main = "", fac = 'Archaeological_Unit')
```

An alternative to the `Momocs::panel()` function is `Momocs::mosaic()`, an updated display function (which will soon replace panel). This does include a legend, unlike the panel function, however as the function is experimental the legend drawing options are limited. 

```{r chunk9, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 5, fig.height = 5}
mosaic(shape, ~Archaeological_Unit, asp = 1, legend = TRUE)
```

We can draw individual shapes using the `Momocs::coo_plot()` function. A number of aesthetic or stylistic changes (including line colour and fill) are also possible. For example:  


```{r chunk10, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 3, fig.height = 3}
coo_plot(shape[4], col = "grey", centroid = TRUE, main = "Artefact #4") 
```

We can also extract measurements from our outline data at this point using the many `Momocs::coo_()` functions. See towards the end of this document for examples.


## **Stage 5: Outline Normalisation**

For today's practical we will perform Elliptic Fourier Analysis. Elliptic Fourier Analysis (EFA) is one of a number of Fourier based methods of curve composition derived from the first series by Jean Baptiste Joseph Fourier (1768-1830), and developed by Giardina and Kuhl (1977) and Kuhl and Giardina (1982). In practice, a set of four parametric equations (grounded on sine and cosine transformations) are used to transform the x and y Cartesian landmarks into curves (Fourier harmonic amplitudes). The coefficients (termed A,B,C and D), when summed together, represent the approximation of artefact form, and are the framework for further analyses. These are comparable to Procrustes coordinates during the GPA procedure.

Normalisation, as stressed by Claude (2008), has long been an issue in the elliptic Fourier process. Normalisation can be performed through the actual elliptic Fourier transformation (using what is known as the "first ellipse"). As we noted in the first workshop, this process (normalisation and elliptic fitting to coefficients) is equivalent to the Procrustes Superimposition for landmark data. 

It is recommended to normalise (standardise) and align your shapes before the `Momocs::efourier()` process. Rotation was considered prior outline digitisation, however rotation could also be explored in Momocs through the `Momocs::coo_aligncalliper()` function prior the `Momocs:efourier()` argument. Here we will perform three transformation processes: 1) `Momocs::coo_center()`, 2) `Momocs::coo_scale()` and 3) `Momocs::coo_close()`. These three functions perform the following actions:  

* `Momocs::coo_center()`: This action centres coordinates on a common origin/common centroid).  

* `Momocs::coo_scale()`: This action scales the coordinates by their 'scale' if provided, or a given centroid size if 'scale is not provided.  
* `Momocs::coo_close()`: Closes unclosed shapes (precautionary).  

```{r chunk11, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
shapenorm <- coo_center(shape)
shapenorm <- coo_scale(shapenorm)
shapenorm <- coo_close(shapenorm)
```

Following this noralisation procedure, we can then use the `Momocs::stack()` function to inspect all outlines, now according to a common centroid and of a common scale:  

```{r chunk12, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 5, fig.height = 5}
stack(shapenorm, title = "Stack: Normalised Outlines")
```

A number of other functions can be performed to normalise these outlines further including `Momocs::coo_slidedirection`, which when shapes are centered configure the first point of the outline to a particular direction. As the Momocs package (?coo_slidedirection) notes: "'right' is possibly the most sensible option (and is by default), since 0 radians points eastwards, relatively to the origin". The `Momocs::coo_untiltx()` function is also advised as to remove rotational biases. The procedure (exemplified using the dplyr piping operators) is as follows:

```{r chunk13, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
shapenorm2 <- shapenorm %>% coo_slidedirection("right") %>% coo_untiltx()
```

When we inspect this stack, the change is apparent:

```{r chunk14, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 5, fig.height = 5}
stack(shapenorm2, title = "Stack: Normalised Outlines with coo_slidedirection()")
```


## **Stage 6: Elliptic Fourier Analysis**

The Giardina and Kuhl parametric equations specify that an outline-specific series of elliptic shapes (of increasing spatial detail) can, when summed together, represent an approximation of the form. This reconstruction of artefact shape can vary depending on the level of accuracy that is required by the researcher; this can be changed depending on the degree of **harmonic power** which is calculated. The first harmonic (first ellipse) is responsible for rotation and defines an ellipse in the plane, with which all other harmonics fit onto. The greater the number of harmonics, the greater the level of detail, and the closer the curves resemble the shape. However, a considerable level of statistical noise is produced if there is too much detail (and thus too many harmonics), and so an appropriate level of harmonics are necessary.  

When a level of harmonic power (shape complexity) is determined by the researcher (95%, 99%, 99.9%, 99.99% shape approximation), a series of procedures can be implemented to test how many harmonics are necessary:  

* `Momocs::calibrate_harmonicpower_efourier()`: This function estimates the number of harmonics required for the elliptic Fourier process (and all other Fourier processes).         
* `Momocs::calibrate_reconstructions_efourier()`: This procedure calculates reconstructed shapes for a series of harmonic numbers. This process best demonstrates the harmonic process.  

* `Momocs::calibrate_deviations_efourier()`: This procedure calculates deviations from the original and reconstructed shapes for a series of harmonic numbers.  

Let's perform these functions on our normalised dataset:

```{r chunk15, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
calibrate_harmonicpower_efourier(shapenorm2, nb.h = 20, plot = FALSE)
```

This first function provides us with the necessary number of harmonics required for the varying degree of shape accuracy. It details (visually and in tabular form) that seventeen harmonics are necessary to achieve 99.9% harmonic power, ten for 99%, six for 95% and five for 90% harmonic powers. This function can require some time to perform, participants can trial the function for an individual object through the addition of "id = x", where x is an artefact number.

```{r chunk16, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
calibrate_reconstructions_efourier(shapenorm2, range = 1:20)
```

This second function calculates and displays reconstructed shapes using a range of harmonic numbers (here coded as the first 20). This best illustrates the reconstruction of shape through varying harmonic powers. Again, we see that by the seventeenth harmonic captures (almost) the entire shape. It is important to note that your results may vary as this function considers a random artefact within the Out object.  

Note: this may vary from artefact to artefact (given their varying complexity) and so the number of harmonics is a guide for the collection of artefacts.  

```{r chunk17, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
calibrate_deviations_efourier(shapenorm2, id = 4)
```

This last function, for a select shape or the range of shapes, calculates deviations from original and reconstructed shapes, along the shape outline, for a range of harmonic numbers.

Once we are satisfied with how many harmonics will be required, we can input this number into the `Momocs::efourier()` function to generated out OutCoe (the outline coefficients). Here we will use 99.9% harmonic power and thus the first seventeen harmonics. 

```{r chunk18, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
efashape <- efourier(shapenorm2, nb.h = 17, smooth.it = 0, norm = TRUE)
```

Technical note: certain artefact shapes may be prone to bad alignment among the first ellipses and result in not-as-ideal homologous coefficients, and in certain instances upside-down (or 180 degrees rotated) shapes on the morphospace (e.g. PCA plots) may occur. It is considered good practice to normalise outlines (as we have done here) and performing the efourier function with `norm = FALSE`.   

Other normalisation procedures not performed here include the addition of landmarks, in order to anchor your artefacts, or through `Momocs::fgProcrustes` through your calliper length.  

`norm = TRUE`, and the use of a numerical alignment ("through the first ellipse"), is also a suitable option following prior normalisation. The degree of prior normalisation is dependent on the complexity of artefact shape and is thus the choice of the researcher.  


## **Stage 7: Principal Component Analysis**  

With our new elliptic Fourier coefficients we can begin the exploratory and analytical procedure. We will start by exploring the main theoretical differences in shape through a **Principal Component Analysis (PCA)**. Please refer to the first workshop for a detailed explanation of PCA, and the second workshop for further examples. In the second workshop we needed to turn our LdkCoe() into a PCA class object through the `Momocs::PCA()` function. Here we need to repeat the process (or as demonstrated previously we can utilise the dplyr piping operators).  

We can then explore the main sources of shape variation through the `Momocs::scree()`, `Momocs::PCcontrib()`, and `Momocs::plot_PCA()` functions.  

```{r chunk19, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
pcashape <- PCA(efashape)
```

With our new PCA class object we can assess the proportion and cumulated proportion of the principal components (sources of shape variation) through `Momocs::scree()`:

```{r chunk20, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
scree(pcashape)
```

Through the scree function we observe that the first seven axes account for 90% cumulative shape variance, eleven axes account for 95% cumulative shape variance, and twenty-four axes account for 99% cumulative shape variance (this will be important to note for subsequent analyses). This can also be visualised as a scree plot:

```{r chunk21, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
scree_plot(pcashape, nax = 1:15)
```

Here we have visualised the first fifteen components, however this can be easily changed through amending the nax argument. We can now investigate what the principal components represent through the `Momocs::PCcontrib()` function:

```{r chunk22, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
PCcontrib(pcashape, nax = 1:7)
```

We can see through this function that Principal Component 1 (PC1), i.e. the main source of shape variation among the tanged points, range from thin tanged points to wider-tanged examples, and that Principal Component 2 (PC2), i.e. the second main source of shape variation, extends from left-exaggerated tangs to right-exaggerated tangs (this shape variance can be removed through flipping all examples on the horizontal axis, however technological profiles were preserved). This function can be set to display as many sources of shape variation as required by the researcher.  

Now we know what each principal component represents, and their values to overall shape variation within our analysis, we can plot our artefacts within a morphospace representative of these components. Here we will use the highly-customisable `Momocs::plot_PCA()` function:  


```{r chunk23, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
plot_PCA(pcashape, axes = c(1,2), ~Archaeological_Unit, morphospace_position = "full_axes", zoom = 2, chull = FALSE) %>% layer_points(cex = 1) %>% layer_ellipses()
```

In this diagram we can observe the different distributions of each archaeological unit within the morphospace, and the relative clustering of each unit within this graph. Some clusters appear incredibly tight, while others are broad, meaning that the archaeological unit is represented by varying shapes, some seen in other units. It's important to remember that this graph only represents the first two principal components, and we may wish to examine other sources of shape variation (some which may be of importance to the archaeological relevance and discriminatory  powerful of shapes).  

Note: pipes (%>%) are used here to processes multiple arguments at the same time. Momocs supports piping with the whole process able to be 'piped'. For teaching purposes we are doing the 'long way' of GMM (as previous).  

If we wish to examine the relationship between different principal components we can use the `axes` argument to change our graph configuration. For example, if we wish to examine differences in shape between PC1 and PC3 we can specify the `axes` argument in the following way:  

```{r chunk24, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
plot_PCA(pcashape, axes = c(1,3), ~Archaeological_Unit, morphospace_position = "full_axes", zoom = 2, chull = FALSE) %>% layer_points(cex = 1) %>% layer_ellipses()
```

The 'morphospace_position' and 'palette' arguments are incredible useful wonderful visualisation tool here, for example:  

```{r chunk25, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
plot_PCA(pcashape, axes = c(1,2), ~Archaeological_Unit, palette = pal_div_PiYG, morphospace_position = "XY", zoom = 1.5, chull = FALSE) %>% layer_points(cex = 3)
```

Note: the `pal_manual()` argument can be added to allow custom palettes.  

Alternatively, we can visualise these principal components, and the variance within different archaeological units through the `Momocs::boxplot()` function. This is particularly useful for capturing variance on many different principal components in one image.  

```{r chunk26, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 9, fig.height = 5}
boxplot(pcashape, ~Archaeological_Unit, nax = 1:5)
```

Note: for all of these images, tidy versions can be provided through minor data wrangling (utilising 'pcashape$x' and a factor).  


## **Stage 8: Discriminant Analysis (LDA/DA/CVA)**  

As we highlighted in the first workshop, PCA explores differences in shape variation irrespective of group composition (*a priori* groupings). Through a discriminant analysis we can examine differences in shape as based on their maximum group separation (between-group variation in contrast to within-group variation). In Momocs, we use the `Momocs::LDA()` function on either the elliptic Fourier coefficients or the PCA scores to produce our class accuracy, plots and correction scores. There is no correct answer as to which to use, it depends on the data you wish to examine e.g. the degree of dimension reductionality. In using the PCA scores it is possible to retain a number of components that are deemed important, this can be either: 1) the first nth components, 2) the number of components representing a certain level of shape variance (e.g. 95%, 99%, 99.9%), or 3) all principal components. The coefficients, in contrast would encapsulate all shape data.  

With greater levels of data you may include a higher degree of unintentional statistical importance, with smaller unimportant variables taking precedence, and so an optimal amount of data is necessary.  

For this practical, let's perform three discriminant analyses on different data: 1) the Fourier coefficients, 2) 95% cumulative shape variance as expressed in PC scores, and 3) 99% cumulative shape variance expressed in PC scores.  

```{r chunk27, echo=TRUE, eval=TRUE, message=FALSE}
dashapefc <- LDA(efashape, ~Archaeological_Unit)
dashape95 <- LDA(pcashape, ~Archaeological_Unit, retain = 0.95)
dashape99 <- LDA(pcashape, ~Archaeological_Unit, retain = 0.99)

dashapefc$CV.correct
dashapefc$CV.ce
dashape95$CV.correct
dashape95$CV.ce
dashape99$CV.correct
dashape99$CV.ce
```

When we examine the Fourier coefficients, a Leave-one-out cross-validation score of 24% (60/250) is obtained, with the Pitted Ware (Type A) and the Grensk forms with the highest class accuracy (37.50% and 30.90% success). With 95% cumulative shape variance a 30% (75/250) classification accuracy is obtained, with higher class accuracies for Pitted Ware and Grensk (54.16% and 36.36%) in addition to considerably higher class accuracy for Bromme (Western Europe) and the Vyshegorian (46.94% and 37.50% respectively), while 99% cumulative shape variance provides a 30.8% (77/250) classification score, with high values for the Pitted Ware, Grensk and Bromme (Western Europe) groups (50.00%, 49.09% and 40.81%). These three discriminant analyses highlight the relative robustness of certain groups, and the weakness of many others, irrespective of how much data is provided. More detailed metrics are included in the `Momocs::classification_metrics()` function (not covered here). Alternatively, these data can be transformed and assessed through further supervised and unsupervised classificatory techniques through tidy machine learning techniques (see the parsnip package for example).  

If we wish to visualise our plot, as is common in exploratory procedures we can use the `Momocs::plot_LDA()` function, using similar arguments to `Momocs::plot_PCA()`. For example, to visualise the discriminant analysis for the Fourier coefficients:  

```{r chunk28, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
plot_LDA(dashapefc, axes = c(1,2), zoom = 1.5, chull = FALSE) %>% layer_points(cex = 1) %>% layer_morphospace_LDA(position = "circle")
```

Note: it is now relatively straight forward to impose the shapes onto the graph using the `layer_morphospace_LDA()` argument.  


## **Stage 9: Multivariate Analysis of Variance (MANOVA)**

So far we have explored the differences in shape within the whole group of artefacts and explored how well they can be separated through their units and their group variance. Now we can test, within an Null Hypothesis Significance Testing (NHST) framework, whether there is difference between the different archaeological units. Again, this can be conducted on the Outline data (Fourier Coefficients) or the PCA scores.  

Once we have chosen a desired alpha level as of marker of difference (that is to say the boundary with which we are able to reject the null hypothesis of same populations) e.g. 0.05 we can use the `Momocs::MANOVA()` function, noting "Archaeological_Unit" to be our factor which we want to consider. For example, through piping, and for the three different methods above: 

```{r chunk29, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
efashape %>% MANOVA(~Archaeological_Unit)

pcashape %>% MANOVA(~Archaeological_Unit, retain = 0.95)

pcashape %>% MANOVA(~Archaeological_Unit, retain = 0.99)
```


In each instance, we can reject the null hypothesis of same populations and infer that there is a statistical difference in the shape of different tanged point archaeological units. This, however, doesn't tell us where the differences lie. For the Principal Component scores we can use the `Momocs::MANOVA_PW()` function:  

```{r chunk30, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
pcashape %>% MANOVA_PW(~Archaeological_Unit, retain = 0.95)

pcashape %>% MANOVA_PW(~Archaeological_Unit, retain = 0.99)
```

This rather large amount of information provides the p values for each combination of archaeological units and depicts level of significance in star form. In terms of analysis this data highlights, as previously the degree to which specific archaeological units can be distinguished from others in terms of their two-dimensional outline shape.  

## **Stage 10: Hierarchical and K-Means Cluster Analysis**  

We can now use the elliptic Fourier coefficients and/or the PCA data to examine, irrespective of previous groupings, how similar objects relate to one another within the overall set of examples. The end-point here will be the construction a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar in two-dimensional outline shape. This can be done through two different methods in Momocs: Hierarchical Cluster Analysis, where the structure is provided, or through a K-Means analysis which partitions the shapes into k groups.  

To perform a Hierarchical Cluster Analysis we can use the `Momocs::CLUST()` function, a wrapper of `stats::dist()` and `stats::hclust()`. We can specify what type of shape we wish for our tree to be using the `type` argument (horizontal as default), and the specific `hclust` (complete as default) and `dist_method` (euclidean as default). Again, we can use the number of PCA scores as we find applicable or use the elliptic Fourier coefficients.  

```{r chunk31, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
CLUST(pcashape, ~Archaeological_Unit, dist_method = "euclidean", hclust_method = "complete", palette = pal_qual, k = 6)
```
This tree can be further examined in the `ape` package and customised further through tree-specific packages e.g. `ggtree`. 

Alternatively we can use the `Momocs::KMEANS()` function to derive four x number of groups from the data. For example, if we wish for four groups:  

```{r chunk32, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
KMEANS(pcashape, centers = 4)
```

More computationally-intensive tree-building exercises can be explored through **maximum likelihood**, using the the `RPhylip` package (this requires the Phylip software to be installed on a computer already).  


## **Stage 11: Constructing Mean Shapes**  

If we wish, we can retrieve mean shapes for a provided factor (e.g. "Archaeological_Unit"), using the elliptic Fourier coefficients or PCA scores. This is done through the `Momocs::MSHAPES()` function with the object first being made.  

```{r chunk33, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
meanshape <- MSHAPES(efashape, ~Archaeological_Unit)
```

The `Momocs::plot_MSHAPES()` function is particularly useful for displaying the mean shapes for all the archaeological units and the visualisation of different configurations of mean shapes.    

```{r chunk34, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 8, fig.height = 8}
plot_MSHAPES(meanshape, size = 0.75)
```

## **Stage 12: Further Exploratory Work**  

Throughout this exercise we have only examined shape, however we still have size data (as all images have a scale). From this we can extract various different measures including length, width, circularity, rectangularity, elongation and symmetry through the `Momocs::coo_()` functions (noting that the converted value for measurements is in pixels and transformation is necessary!). We could take any of these measures and examine this new factor against our data.  

Centroid size is perhaps the best measure of size, incorporating the distance from all points of interest in relation to the shape. This can be extracted from the original shape data, using the `Momocs::coo_centsize()` function. We can then take this data and the principal component scores, and merge them into one database. There are a variety of ways this can be done, this is just one example.  

```{r chunk35, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
centroidsize <- as_tibble(coo_centsize(shape))
centroidsize <- rename(centroidsize, CS = "value")
pcascores <- as_tibble(pcashape$x)
databasedata <- cbind(database,centroidsize, pcascores)
```

We can now explore these through regression and correlation-based analyses. For example, using the ggplot functions we can create a scatter plot and add a regression line. As a demonstration, let's compare the first source of variation (PC1) with centroid size:  

```{r chunk36, echo=TRUE, eval=TRUE, message=FALSE,warning=FALSE}
ggplot(databasedata, aes(PC1, CS)) + geom_point(size = 2, pch = 16, alpha = 0.4, colour = "#E69F00", fill = "#ffd475") + geom_smooth(method=lm, se=FALSE) + theme(text = element_text(size=8), axis.text = element_text(size = 8)) + xlab("Principal Component 1") + ylab("CS (Centroid Size)")
```

We can then perform a correlation (and test) using the `cor` and `cor.test` functions:    

```{r chunk37, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
cor(databasedata$PC1, databasedata$CS)
cor.test(databasedata$PC1, databasedata$CS)
```

## **Concluding Remarks**

This workshop was designed to highlight how geometric morphometrics (outline analysis) can be examined for archaeological material in the R Environment, from data importing to visualisation and analysis. It is worth stressing that Momocs is only one of a number of packages in the R Environment, and the methods showcased here are only one way (and one style) of conducting GMM. Only through exploring the packages and their functions will you be able understand what workflow works best for your research question and process.  

If there are any questions please feel free to contact me: C.S.Hoggard@soton.ac.uk  

For literature pertaining to GMM by the author (including code and data) see:  

* Hoggard, C.S., Lauridsen, L. and Witte, K.B. (2019). The Potential of Geometric Morphometrics for Danish Archaeology: Two Case Studies. *Arkaeologisk Forum*, 40:  30-42. (http://www.archaeology.dk/16738/Nr.%2040%20-%202019).  OSF: https://osf.io/en5d2/.      

* Hoggard, C.S., McNabb, J. and Cole, J.N. (2019). The application of elliptic Fourier analysis in understanding biface shape and symmetry through the British Acheulean. *Journal of Paleolithic Archaeology*, 2 (2): 115-133. (https://doi.org/10.1007/s41982-019-00024-6). OSF: https://osf.io/td92j/.  

* Ivanovaite, L., Swertka, K., Hoggard, C.S., Sauer, F. and Riede, F. (2020). All these fantastic cultures? Research history and regionalisation in the Late Palaeolithic tanged point cultures of Eastern Europe. *European Journal of Archaeology*. (https://doi.org/10.1017/eaa.2019.59).  OSF: https://osf.io/agrwb/.  

* Vestergaard, C. and Hoggard, C.S. (2019). A Novel Geometric Morphometric (GMM) Application to the Study of Bronze Age Tutuli. *Danish Journal of Archaeology*, 8: 5-28. (https://tidsskrift.dk/dja/article/view/112494/164318).  OSF: https://osf.io/fcp43/.  

* Riede, F., Hoggard, C.S. and Shennan, S. (2019). Reconciling material cultures in archaeology with genetic data requires robust cultural evolutionary taxonomies. *Nature: Palgrave Communications*, 5 (1): 55. (https://doi.org/10.1057/s41599-019-0260-7). OSF: https://osf.io/vtdf2/.  

